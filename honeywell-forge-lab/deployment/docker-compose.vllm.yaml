# Honeywell Forge Cognition - vLLM Backend Deployment
# Uses real GPU inference with open-source model for prototype testing
#
# This demonstrates real inference performance without Honeywell's model.
# When Honeywell provides their model, we swap the MODEL_NAME.
#
# Requirements:
#   - NVIDIA GPU with sufficient VRAM (see model options below)
#   - nvidia-container-toolkit installed
#   - Docker with NVIDIA runtime
#
# Model Options (adjust based on your GPU):
#   - TinyLlama/TinyLlama-1.1B-Chat-v1.0  (~3GB VRAM, Tesla P40 OK)
#   - microsoft/phi-2                      (~6GB VRAM, Tesla P40 OK)
#   - meta-llama/Llama-2-7b-chat-hf       (~14GB VRAM, needs 16GB+)
#   - mistralai/Mistral-7B-Instruct-v0.2  (~14GB VRAM, needs 16GB+)
#
# Usage:
#   # Start with default model (TinyLlama)
#   docker-compose -f docker-compose.vllm.yaml up -d
#
#   # Start with specific model
#   MODEL_NAME=microsoft/phi-2 docker-compose -f docker-compose.vllm.yaml up -d
#
#   # View logs
#   docker-compose -f docker-compose.vllm.yaml logs -f vllm-server
#
#   # Test inference
#   curl http://localhost:8000/v1/completions \
#     -H "Content-Type: application/json" \
#     -d '{"model": "default", "prompt": "What is HVAC maintenance?", "max_tokens": 100}'

version: '3.8'

services:
  # ==========================================================================
  # vLLM Inference Server (Real GPU Inference)
  # ==========================================================================
  vllm-server:
    image: vllm/vllm-openai:latest
    container_name: forge-vllm
    restart: unless-stopped
    ports:
      - "8000:8000"   # OpenAI-compatible API
    environment:
      # Model configuration
      - MODEL_NAME=${MODEL_NAME:-TinyLlama/TinyLlama-1.1B-Chat-v1.0}
      # vLLM settings
      - VLLM_TENSOR_PARALLEL_SIZE=${TENSOR_PARALLEL_SIZE:-1}
      - VLLM_GPU_MEMORY_UTILIZATION=${GPU_MEMORY_UTILIZATION:-0.85}
      - VLLM_MAX_MODEL_LEN=${MAX_MODEL_LEN:-4096}
      # HuggingFace cache
      - HF_HOME=/models
      - TRANSFORMERS_CACHE=/models
    volumes:
      # Model cache - persists downloaded models
      - model-cache:/models
      # Optional: Mount local model files
      # - ./models:/local-models:ro
    command: >
      --model ${MODEL_NAME:-TinyLlama/TinyLlama-1.1B-Chat-v1.0}
      --tensor-parallel-size ${TENSOR_PARALLEL_SIZE:-1}
      --gpu-memory-utilization ${GPU_MEMORY_UTILIZATION:-0.85}
      --max-model-len ${MAX_MODEL_LEN:-4096}
      --trust-remote-code
      --host 0.0.0.0
      --port 8000
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
        limits:
          memory: ${MEMORY_LIMIT:-32G}
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 300s   # Model loading can take 2-5 minutes
    logging:
      driver: json-file
      options:
        max-size: "100m"
        max-file: "5"
    # Ulimits for high-performance
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536

  # ==========================================================================
  # Our Wrapper API (Session Management + Metrics)
  # ==========================================================================
  forge-api:
    image: ${REGISTRY:-forge}/inference-server:${VERSION:-latest}
    build:
      context: ../inference-server
      dockerfile: Dockerfile
    container_name: forge-api
    restart: unless-stopped
    ports:
      - "8080:8000"   # Our API (different port to avoid conflict)
    environment:
      # Use OpenAI-compatible backend pointing to vLLM
      - FORGE_INFERENCE_BACKEND=openai_compatible
      - FORGE_BACKEND_URL=http://vllm-server:8000/v1
      - FORGE_MODEL_NAME=${MODEL_NAME:-TinyLlama/TinyLlama-1.1B-Chat-v1.0}
      # SKU detection
      - FORGE_SKU_AUTO_DETECT=true
      - FORGE_SKU=${FORGE_SKU:-}
    depends_on:
      vllm-server:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # ==========================================================================
  # GPU Metrics Exporter
  # ==========================================================================
  gpu-exporter:
    image: ${REGISTRY:-nvidia}/dcgm-exporter:3.3.0-3.2.0-ubuntu22.04
    container_name: forge-gpu-exporter
    restart: unless-stopped
    ports:
      - "9400:9400"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    environment:
      - DCGM_EXPORTER_LISTEN=:9400
      - DCGM_EXPORTER_KUBERNETES=false

  # ==========================================================================
  # Prometheus (Metrics Collection)
  # ==========================================================================
  prometheus:
    image: prom/prometheus:v2.48.0
    container_name: forge-prometheus
    restart: unless-stopped
    ports:
      - "9091:9090"
    volumes:
      - ./prometheus-vllm.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=7d'

  # ==========================================================================
  # Grafana (Dashboards)
  # ==========================================================================
  grafana:
    image: grafana/grafana:10.2.0
    container_name: forge-grafana
    restart: unless-stopped
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD:-admin}
      - GF_USERS_ALLOW_SIGN_UP=false
    volumes:
      - grafana-data:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning:ro
    depends_on:
      - prometheus

volumes:
  model-cache:
    driver: local
  prometheus-data:
    driver: local
  grafana-data:
    driver: local

networks:
  default:
    name: forge-vllm-network
