apiVersion: batch/v1
kind: Job
metadata:
  name: gpu-test-job
  labels:
    app: gpu-test
    demo: rosa-gpu-lab
spec:
  backoffLimit: 0
  ttlSecondsAfterFinished: 300
  template:
    metadata:
      labels:
        app: gpu-test
    spec:
      restartPolicy: Never

      containers:
      - name: gpu-test
        # PyTorch image with CUDA support
        image: pytorch/pytorch:2.1.0-cuda12.1-cudnn8-runtime

        command:
        - /bin/bash
        - -c
        - |
          echo "=============================================="
          echo "  ROSA GPU TEST JOB"
          echo "=============================================="
          echo "Start Time: $(date)"
          echo "Pod Name: $HOSTNAME"
          echo ""

          echo "--- Step 1: Check GPU Availability ---"
          if nvidia-smi; then
            echo ""
            echo "[SUCCESS] GPUs detected!"
          else
            echo ""
            echo "[ERROR] No GPUs found or nvidia-smi failed"
            exit 1
          fi

          echo ""
          echo "--- Step 2: GPU Memory Info ---"
          nvidia-smi --query-gpu=index,name,memory.total,memory.free --format=csv

          echo ""
          echo "--- Step 3: Running GPU Computation ---"
          python3 -c "
          import torch
          import time

          print('PyTorch version:', torch.__version__)
          print('CUDA available:', torch.cuda.is_available())
          print('CUDA version:', torch.version.cuda)
          print('GPU device:', torch.cuda.get_device_name(0))
          print('')

          # Create tensors on GPU
          size = 3000
          print(f'Creating {size}x{size} matrices on GPU...')
          a = torch.randn(size, size, device='cuda')
          b = torch.randn(size, size, device='cuda')

          # Warm up
          c = torch.matmul(a, b)
          torch.cuda.synchronize()

          # Benchmark matrix multiplication
          print('Running matrix multiplication benchmark...')
          start = time.time()
          for i in range(20):
              c = torch.matmul(a, b)
          torch.cuda.synchronize()
          elapsed = time.time() - start

          print('')
          print('=== Results ===')
          print(f'Matrix size: {size}x{size}')
          print(f'Operations: 20 matrix multiplications')
          print(f'Total time: {elapsed:.2f} seconds')
          print(f'TFLOPS: {(2 * size**3 * 20) / elapsed / 1e12:.2f}')
          print('')
          print('GPU computation completed successfully!')
          "

          echo ""
          echo "=============================================="
          echo "  GPU TEST COMPLETED SUCCESSFULLY"
          echo "=============================================="
          echo "End Time: $(date)"
          echo ""

        resources:
          limits:
            nvidia.com/gpu: 1
            memory: "4Gi"
            cpu: "2"
          requests:
            nvidia.com/gpu: 1
            memory: "2Gi"
            cpu: "1"

        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL

      tolerations:
      - key: "nvidia.com/gpu"
        operator: "Exists"
        effect: "NoSchedule"
